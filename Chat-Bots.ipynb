{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer Chat Bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will be working with the Babi Data Set from Facebook Research.\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Format of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setting up Vocabulary of All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set that holds the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    # In case you don't know what a union of sets is:\n",
    "    # https://www.programiz.com/python-programming/methods/set/union\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for pad_sequences\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bedroom': 1,\n",
       " 'back': 2,\n",
       " 'took': 3,\n",
       " 'moved': 4,\n",
       " 'grabbed': 5,\n",
       " 'got': 6,\n",
       " 'picked': 7,\n",
       " 'journeyed': 8,\n",
       " 'up': 9,\n",
       " 'the': 10,\n",
       " 'john': 11,\n",
       " 'apple': 12,\n",
       " '?': 13,\n",
       " 'dropped': 14,\n",
       " 'milk': 15,\n",
       " 'went': 16,\n",
       " 'travelled': 17,\n",
       " 'is': 18,\n",
       " 'mary': 19,\n",
       " 'to': 20,\n",
       " 'office': 21,\n",
       " 'kitchen': 22,\n",
       " 'garden': 23,\n",
       " 'no': 24,\n",
       " 'yes': 25,\n",
       " 'there': 26,\n",
       " 'hallway': 27,\n",
       " 'put': 28,\n",
       " 'daniel': 29,\n",
       " 'in': 30,\n",
       " '.': 31,\n",
       " 'discarded': 32,\n",
       " 'bathroom': 33,\n",
       " 'sandra': 34,\n",
       " 'football': 35,\n",
       " 'left': 36,\n",
       " 'down': 37}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalize Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories,Queries,and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
    "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
    "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n",
    "        #\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story,query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 10,  1, 31],\n",
       "       [ 0,  0,  0, ..., 10, 23, 31],\n",
       "       [ 0,  0,  0, ..., 10, 23, 31],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 10, 12, 31],\n",
       "       [ 0,  0,  0, ..., 10, 23, 31],\n",
       "       [ 0,  0,  0, ..., 12, 26, 31]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 11, 30, 10, 22, 13],\n",
       "       [18, 11, 30, 10, 22, 13],\n",
       "       [18, 11, 30, 10, 23, 13],\n",
       "       ...,\n",
       "       [18, 19, 30, 10,  1, 13],\n",
       "       [18, 34, 30, 10, 23, 13],\n",
       "       [18, 19, 30, 10, 23, 13]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 503., 497.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for Inputs\n",
    "\n",
    "Recall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building the Networks\n",
    "\n",
    "To understand why we chose this setup, make sure to read the paper we are using:\n",
    "\n",
    "* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "### Input Encoder m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shubh\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Input gets embedded to a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# (samples, story_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use dot product to compute the match between first input vector seq and the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add this match matrix with the second input vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 6, 220) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce with RNN (LSTM)\n",
    "answer = LSTM(32)(answer)  # (samples, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shubh\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\shubh\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.9425 - accuracy: 0.4935 - val_loss: 0.6959 - val_accuracy: 0.5030\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 7s 734us/step - loss: 0.7042 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.4700\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 7s 708us/step - loss: 0.6962 - accuracy: 0.5025 - val_loss: 0.6937 - val_accuracy: 0.5030\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 7s 713us/step - loss: 0.6953 - accuracy: 0.4960 - val_loss: 0.6953 - val_accuracy: 0.4970\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 7s 735us/step - loss: 0.6946 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.4640\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 7s 730us/step - loss: 0.6947 - accuracy: 0.4977 - val_loss: 0.6950 - val_accuracy: 0.4970\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 8s 755us/step - loss: 0.6951 - accuracy: 0.4904 - val_loss: 0.6959 - val_accuracy: 0.4970\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 7s 731us/step - loss: 0.6942 - accuracy: 0.5003 - val_loss: 0.6941 - val_accuracy: 0.4960\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 7s 728us/step - loss: 0.6941 - accuracy: 0.5102 - val_loss: 0.6985 - val_accuracy: 0.4970\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 7s 723us/step - loss: 0.6930 - accuracy: 0.5123 - val_loss: 0.6961 - val_accuracy: 0.5030\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 7s 737us/step - loss: 0.6911 - accuracy: 0.5138 - val_loss: 0.6945 - val_accuracy: 0.4850\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 7s 722us/step - loss: 0.6862 - accuracy: 0.5389 - val_loss: 0.6963 - val_accuracy: 0.5150\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 8s 784us/step - loss: 0.6739 - accuracy: 0.5778 - val_loss: 0.6835 - val_accuracy: 0.5520\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 8s 809us/step - loss: 0.6636 - accuracy: 0.6038 - val_loss: 0.6566 - val_accuracy: 0.6320\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 8s 770us/step - loss: 0.6541 - accuracy: 0.6182 - val_loss: 0.6458 - val_accuracy: 0.6500\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 7s 671us/step - loss: 0.6450 - accuracy: 0.6323 - val_loss: 0.6428 - val_accuracy: 0.6320\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 6s 644us/step - loss: 0.6316 - accuracy: 0.6467 - val_loss: 0.6191 - val_accuracy: 0.6620\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 6s 636us/step - loss: 0.6148 - accuracy: 0.6672 - val_loss: 0.6391 - val_accuracy: 0.6320\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 7s 745us/step - loss: 0.5944 - accuracy: 0.6890 - val_loss: 0.5827 - val_accuracy: 0.7110\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 7s 687us/step - loss: 0.5709 - accuracy: 0.7124 - val_loss: 0.5433 - val_accuracy: 0.7320\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 7s 745us/step - loss: 0.5582 - accuracy: 0.7209 - val_loss: 0.5538 - val_accuracy: 0.7320\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 7s 715us/step - loss: 0.5314 - accuracy: 0.7467 - val_loss: 0.5091 - val_accuracy: 0.7650\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 7s 732us/step - loss: 0.5168 - accuracy: 0.7603 - val_loss: 0.4924 - val_accuracy: 0.7620\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 7s 659us/step - loss: 0.4937 - accuracy: 0.7747 - val_loss: 0.4695 - val_accuracy: 0.7840\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 7s 692us/step - loss: 0.4788 - accuracy: 0.7865 - val_loss: 0.4684 - val_accuracy: 0.7870\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 10s 988us/step - loss: 0.4590 - accuracy: 0.7941 - val_loss: 0.4470 - val_accuracy: 0.8040\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 7s 651us/step - loss: 0.4472 - accuracy: 0.8037 - val_loss: 0.4266 - val_accuracy: 0.8030\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 7s 662us/step - loss: 0.4325 - accuracy: 0.8125 - val_loss: 0.4132 - val_accuracy: 0.8090\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.4147 - accuracy: 0.8215 - val_loss: 0.4012 - val_accuracy: 0.8170\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 7s 738us/step - loss: 0.4013 - accuracy: 0.8291 - val_loss: 0.4023 - val_accuracy: 0.8210\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 7s 749us/step - loss: 0.3919 - accuracy: 0.8329 - val_loss: 0.3912 - val_accuracy: 0.8250\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 7s 699us/step - loss: 0.3789 - accuracy: 0.8431 - val_loss: 0.3885 - val_accuracy: 0.8230\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 7s 695us/step - loss: 0.3686 - accuracy: 0.8459 - val_loss: 0.3830 - val_accuracy: 0.8310\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 7s 685us/step - loss: 0.3667 - accuracy: 0.8451 - val_loss: 0.3752 - val_accuracy: 0.8250\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 7s 682us/step - loss: 0.3597 - accuracy: 0.8489 - val_loss: 0.3755 - val_accuracy: 0.8270\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 7s 679us/step - loss: 0.3559 - accuracy: 0.8491 - val_loss: 0.3682 - val_accuracy: 0.8250\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 7s 681us/step - loss: 0.3534 - accuracy: 0.8521 - val_loss: 0.3806 - val_accuracy: 0.8270\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 7s 682us/step - loss: 0.3488 - accuracy: 0.8523 - val_loss: 0.3706 - val_accuracy: 0.8260\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 7s 687us/step - loss: 0.3476 - accuracy: 0.8575 - val_loss: 0.3633 - val_accuracy: 0.8290\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 7s 694us/step - loss: 0.3454 - accuracy: 0.8498 - val_loss: 0.3630 - val_accuracy: 0.8290\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 7s 679us/step - loss: 0.3370 - accuracy: 0.8602 - val_loss: 0.3613 - val_accuracy: 0.8370\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 7s 685us/step - loss: 0.3373 - accuracy: 0.8556 - val_loss: 0.3762 - val_accuracy: 0.8240\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 7s 686us/step - loss: 0.3355 - accuracy: 0.8602 - val_loss: 0.3697 - val_accuracy: 0.8340\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 7s 704us/step - loss: 0.3306 - accuracy: 0.8593 - val_loss: 0.3959 - val_accuracy: 0.8200\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 7s 694us/step - loss: 0.3273 - accuracy: 0.8631 - val_loss: 0.3741 - val_accuracy: 0.8280\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 7s 687us/step - loss: 0.3228 - accuracy: 0.8651 - val_loss: 0.3805 - val_accuracy: 0.8330\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 7s 689us/step - loss: 0.3223 - accuracy: 0.8654 - val_loss: 0.3721 - val_accuracy: 0.8360\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 7s 687us/step - loss: 0.3179 - accuracy: 0.8640 - val_loss: 0.3652 - val_accuracy: 0.8290\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 7s 706us/step - loss: 0.3204 - accuracy: 0.8623 - val_loss: 0.3895 - val_accuracy: 0.8290\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 7s 688us/step - loss: 0.3150 - accuracy: 0.8641 - val_loss: 0.3660 - val_accuracy: 0.8260\n",
      "Epoch 51/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 679us/step - loss: 0.3210 - accuracy: 0.8629 - val_loss: 0.3515 - val_accuracy: 0.8370\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 7s 674us/step - loss: 0.3178 - accuracy: 0.8640 - val_loss: 0.3710 - val_accuracy: 0.8300\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 7s 675us/step - loss: 0.3135 - accuracy: 0.8677 - val_loss: 0.3757 - val_accuracy: 0.8330\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 7s 671us/step - loss: 0.3116 - accuracy: 0.8693 - val_loss: 0.3918 - val_accuracy: 0.8330\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 7s 673us/step - loss: 0.3120 - accuracy: 0.8679 - val_loss: 0.3749 - val_accuracy: 0.8320\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 7s 676us/step - loss: 0.3081 - accuracy: 0.8709 - val_loss: 0.3760 - val_accuracy: 0.8290\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 7s 678us/step - loss: 0.3032 - accuracy: 0.8684 - val_loss: 0.3819 - val_accuracy: 0.8200\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 7s 678us/step - loss: 0.3058 - accuracy: 0.8710 - val_loss: 0.3937 - val_accuracy: 0.8210\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 7s 673us/step - loss: 0.3056 - accuracy: 0.8712 - val_loss: 0.3859 - val_accuracy: 0.8310\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 7s 678us/step - loss: 0.3032 - accuracy: 0.8695 - val_loss: 0.3790 - val_accuracy: 0.8340\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 7s 682us/step - loss: 0.3021 - accuracy: 0.8699 - val_loss: 0.3676 - val_accuracy: 0.8340\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 7s 672us/step - loss: 0.2990 - accuracy: 0.8738 - val_loss: 0.3983 - val_accuracy: 0.8290\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 7s 677us/step - loss: 0.2983 - accuracy: 0.8699 - val_loss: 0.3885 - val_accuracy: 0.8260\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 7s 675us/step - loss: 0.2946 - accuracy: 0.8735 - val_loss: 0.3897 - val_accuracy: 0.8220\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 7s 681us/step - loss: 0.2932 - accuracy: 0.8720 - val_loss: 0.3799 - val_accuracy: 0.8220\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 7s 674us/step - loss: 0.2971 - accuracy: 0.8739 - val_loss: 0.3729 - val_accuracy: 0.8280\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 7s 696us/step - loss: 0.2975 - accuracy: 0.8719 - val_loss: 0.3887 - val_accuracy: 0.8210\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 7s 679us/step - loss: 0.2943 - accuracy: 0.8742 - val_loss: 0.3736 - val_accuracy: 0.8250\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 7s 678us/step - loss: 0.2886 - accuracy: 0.8770 - val_loss: 0.3750 - val_accuracy: 0.8280\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 7s 676us/step - loss: 0.2964 - accuracy: 0.8786 - val_loss: 0.3833 - val_accuracy: 0.8290\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 7s 673us/step - loss: 0.2888 - accuracy: 0.8807 - val_loss: 0.3665 - val_accuracy: 0.8260\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 7s 675us/step - loss: 0.2833 - accuracy: 0.8795 - val_loss: 0.3877 - val_accuracy: 0.8330\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 7s 681us/step - loss: 0.2810 - accuracy: 0.8818 - val_loss: 0.3933 - val_accuracy: 0.8350\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 7s 700us/step - loss: 0.2857 - accuracy: 0.8805 - val_loss: 0.4014 - val_accuracy: 0.8330\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 7s 697us/step - loss: 0.2863 - accuracy: 0.8783 - val_loss: 0.4288 - val_accuracy: 0.8290\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 7s 700us/step - loss: 0.2832 - accuracy: 0.8824 - val_loss: 0.3917 - val_accuracy: 0.8280\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 7s 720us/step - loss: 0.2851 - accuracy: 0.8791 - val_loss: 0.4054 - val_accuracy: 0.8190\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 9s 880us/step - loss: 0.2796 - accuracy: 0.8805 - val_loss: 0.4480 - val_accuracy: 0.8300\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 7s 714us/step - loss: 0.2753 - accuracy: 0.8833 - val_loss: 0.3974 - val_accuracy: 0.8240\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 7s 686us/step - loss: 0.2778 - accuracy: 0.8790 - val_loss: 0.3830 - val_accuracy: 0.8270\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 8s 757us/step - loss: 0.2828 - accuracy: 0.8812 - val_loss: 0.3912 - val_accuracy: 0.8310\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 9s 928us/step - loss: 0.2738 - accuracy: 0.8823 - val_loss: 0.3956 - val_accuracy: 0.8310\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 7s 666us/step - loss: 0.2798 - accuracy: 0.8794 - val_loss: 0.3918 - val_accuracy: 0.8270\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 7s 661us/step - loss: 0.2731 - accuracy: 0.8865 - val_loss: 0.4037 - val_accuracy: 0.8240\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 7s 657us/step - loss: 0.2670 - accuracy: 0.8819 - val_loss: 0.4468 - val_accuracy: 0.8290\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 7s 662us/step - loss: 0.2786 - accuracy: 0.8821 - val_loss: 0.4052 - val_accuracy: 0.8170\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 7s 651us/step - loss: 0.2723 - accuracy: 0.8827 - val_loss: 0.4175 - val_accuracy: 0.8220\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 7s 698us/step - loss: 0.2728 - accuracy: 0.8839 - val_loss: 0.4149 - val_accuracy: 0.8120\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 8s 798us/step - loss: 0.2684 - accuracy: 0.8864 - val_loss: 0.4252 - val_accuracy: 0.8150\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 6s 641us/step - loss: 0.2668 - accuracy: 0.8865 - val_loss: 0.4157 - val_accuracy: 0.8160\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 7s 698us/step - loss: 0.2507 - accuracy: 0.8921 - val_loss: 0.4381 - val_accuracy: 0.8260\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 8s 796us/step - loss: 0.2522 - accuracy: 0.8940 - val_loss: 0.4348 - val_accuracy: 0.8180\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 8s 756us/step - loss: 0.2502 - accuracy: 0.8966 - val_loss: 0.4602 - val_accuracy: 0.8140\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 7s 669us/step - loss: 0.2509 - accuracy: 0.8950 - val_loss: 0.4628 - val_accuracy: 0.8180\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 7s 676us/step - loss: 0.2478 - accuracy: 0.8941 - val_loss: 0.4318 - val_accuracy: 0.8210\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.2366 - accuracy: 0.8990 - val_loss: 0.4588 - val_accuracy: 0.8230\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 8s 799us/step - loss: 0.2415 - accuracy: 0.8967 - val_loss: 0.4491 - val_accuracy: 0.8190\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 8s 821us/step - loss: 0.2447 - accuracy: 0.8955 - val_loss: 0.4573 - val_accuracy: 0.8160\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 7s 725us/step - loss: 0.2477 - accuracy: 0.8963 - val_loss: 0.4589 - val_accuracy: 0.8210\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 7s 716us/step - loss: 0.2489 - accuracy: 0.8934 - val_loss: 0.4596 - val_accuracy: 0.8230\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 6s 647us/step - loss: 0.2433 - accuracy: 0.8997 - val_loss: 0.4982 - val_accuracy: 0.8150\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 6s 649us/step - loss: 0.2350 - accuracy: 0.9018 - val_loss: 0.4312 - val_accuracy: 0.8270\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 7s 682us/step - loss: 0.2377 - accuracy: 0.9016 - val_loss: 0.4716 - val_accuracy: 0.8230\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 6s 637us/step - loss: 0.2352 - accuracy: 0.9009 - val_loss: 0.4893 - val_accuracy: 0.8130\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 644us/step - loss: 0.2360 - accuracy: 0.9020 - val_loss: 0.4959 - val_accuracy: 0.8270\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 7s 668us/step - loss: 0.2389 - accuracy: 0.8973 - val_loss: 0.4915 - val_accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chatbot_120_epochs.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "### Plotting Out Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Given Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'got',\n",
       " 'the',\n",
       " 'milk',\n",
       " 'there',\n",
       " '.',\n",
       " 'John',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary got the milk there . John moved to the bedroom .\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[0][0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is John in the kitchen ?\n"
     ]
    }
   ],
   "source": [
    "query = ' '.join(word for word in test_data[0][1])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Test Answer from Data is: no\n"
     ]
    }
   ],
   "source": [
    "print(\"True Test Answer from Data is:\",test_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9999825\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Own Stories and Questions\n",
    "\n",
    "We can only use words from the existing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'left',\n",
       " 'the',\n",
       " 'kitchen',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'dropped',\n",
       " 'the',\n",
       " 'football',\n",
       " 'in',\n",
       " 'the',\n",
       " 'garden',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the whitespace of the periods\n",
    "my_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\n",
    "my_story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"Is the football in the garden ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'the', 'football', 'in', 'the', 'garden', '?']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(),my_question.split(),'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story,my_ques,my_ans = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  yes\n",
      "Probability of certainty was:  0.9941268\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
